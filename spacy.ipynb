{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n",
    "https://spacy.io/\n",
    "\n",
    "A good NLP library in Python.\n",
    "\n",
    "Main features: \n",
    "* Non-destructive tokenization\n",
    "* Named entity recognition\n",
    "* Support for over 25 languages\n",
    "* Statistical models models for 8 languages\n",
    "* Pre-trained word vectors\n",
    "* Part-of-speech tagging\n",
    "* Labelled dependency parsing\n",
    "* Syntax-driven sentence segmentation\n",
    "* Text classification\n",
    "* Built-in visualizers for syntax and named entities\n",
    "* Deep learning integration\n",
    "\n",
    "Many alternatives, incl. NLTK (https://www.nltk.org/book/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'done loading'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy # conda install -c conda-forge spacy (or https://spacy.io/usage/)\n",
    "\n",
    "# download models:\n",
    "# python -m spacy.en.download\n",
    "# for a bigger one: python -m spacy.en_core_web_sm.download\n",
    "# german: python -m spacy.de.download (see https://spacy.io/models/de#de_core_news_sm)\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "'done loading'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DET'),\n",
       " ('is', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('sentence', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(u\"This is a sentence.\")\n",
    "[(w.text, w.pos_) for w in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.K.\n",
      "startup\n",
      "for\n",
      "$\n",
      "1\n",
      "billion\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__bytes__   __class__   __delattr__   __dir__   __doc__   __eq__   __format__   __ge__   __getattribute__   __gt__   __hash__   __init__   __init_subclass__   __le__   __len__   __lt__   __ne__   __new__   __pyx_vtable__   __reduce__   __reduce_ex__   __repr__   __setattr__   __sizeof__   __str__   __subclasshook__   __unicode__   ancestors   check_flag   children   cluster   conjuncts   dep   dep_   doc   ent_iob   ent_iob_   ent_type   ent_type_   has_vector   head   i   idx   is_alpha   is_ancestor_of   is_ascii   is_bracket   is_digit   is_left_punct   is_lower   is_oov   is_punct   is_quote   is_right_punct   is_space   is_stop   is_title   lang   lang_   left_edge   lefts   lemma   lemma_   lex_id   like_email   like_num   like_url   lower   lower_   n_lefts   n_rights   nbor   norm   norm_   orth   orth_   pos   pos_   prefix   prefix_   prob   rank   repvec   right_edge   rights   shape   shape_   similarity   string   subtree   suffix   suffix_   tag   tag_   text   text_with_ws   vector   vector_norm   whitespace_'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# many things happening under the hood:\n",
    "'   '.join(dir(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'displacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6d718548a19a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# visualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dep'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'displacy'"
     ]
    }
   ],
   "source": [
    "# visualize\n",
    "from spacy import displacy\n",
    "displacy.serve(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6930155149452184\n"
     ]
    }
   ],
   "source": [
    "# Determine semantic similarities\n",
    "doc1 = nlp(u\"my fries were super gross\")\n",
    "doc2 = nlp(u\"such disgusting fries\")\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris',\n",
       " 'paris',\n",
       " 'PARIS',\n",
       " 'Strasbourg',\n",
       " 'strasbourg',\n",
       " 'STRASBOURG',\n",
       " 'Brussels',\n",
       " 'brussels',\n",
       " 'BRUSSELS',\n",
       " 'Rouen',\n",
       " 'rouen',\n",
       " 'ROUEN',\n",
       " 'Marseilles',\n",
       " 'marseilles',\n",
       " 'MARSEILLES',\n",
       " 'Lausanne',\n",
       " 'LAUSANNE',\n",
       " 'lausanne',\n",
       " 'AIX-EN-PROVENCE',\n",
       " 'Aix-en-Provence']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(word):\n",
    "    by_similarity = sorted(word.vocab, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [w.orth_ for w in by_similarity[:20]]\n",
    "\n",
    "most_similar(nlp.vocab[u'Paris'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris',\n",
       " 'cologne',\n",
       " 'bologna',\n",
       " 'rome',\n",
       " 'london',\n",
       " 'madrid',\n",
       " 'tokyo',\n",
       " 'liege',\n",
       " 'sens',\n",
       " 'okc',\n",
       " 'france',\n",
       " 'vegas',\n",
       " 'montreal',\n",
       " 'nyc',\n",
       " 'italy',\n",
       " 'tienen',\n",
       " 'pouvoir',\n",
       " 'toronto',\n",
       " 'gent',\n",
       " 'ganja']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:20]\n",
    "\n",
    "[w.lower_ for w in most_similar(nlp.vocab[u'paris'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
